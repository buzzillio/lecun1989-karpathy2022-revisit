# ============================================
# RaMCTS suite with Taxi-v3 (deterministic)
# ============================================

import math, random, collections
from typing import Tuple, Dict, List, Any, Optional
import numpy as np, pandas as pd
import gymnasium as gym

GLOBAL_SEED = 12345
random.seed(GLOBAL_SEED); np.random.seed(GLOBAL_SEED)

# ------------------------------
# Lightweight Environment Models
# ------------------------------
class BaseEnvModel:
    action_count: int
    def step(self, state, action) -> Tuple[int, float, bool]:
        raise NotImplementedError

class FrozenLakeModel(BaseEnvModel):
    def __init__(self, map_name="4x4"):
        if map_name == "4x4":
            self.desc, self.map_size = ["SFFF", "FHFH", "FFFH", "HFFG"], 4
        elif map_name == "8x8":
            self.desc, self.map_size = [
                "SFFFFFFF","FFFFFFFF","FFFHFFFF","FFFFFHFF",
                "FFFHFFFF","FHHFFFHF","FHFFHFHF","FFFHFFFG"
            ], 8
        else:
            raise ValueError("map_name must be '4x4' or '8x8'")
        self.holes = {r*self.map_size+c for r,row in enumerate(self.desc)
                      for c,ch in enumerate(row) if ch=='H'}
        self.goal_state = self.map_size*self.map_size-1
        self.action_count = 4

    def step(self, state, action):
        r,c = divmod(state, self.map_size)
        if action == 0: c = max(c-1, 0)                   # L
        elif action == 1: r = min(r+1, self.map_size-1)   # D
        elif action == 2: c = min(c+1, self.map_size-1)   # R
        elif action == 3: r = max(r-1, 0)                 # U
        ns = r*self.map_size + c
        if ns in self.holes: return ns, 0.0, True
        if ns == self.goal_state: return ns, 1.0, True
        return ns, 0.0, False

class CliffWalkingModel(BaseEnvModel):
    def __init__(self):
        self.shape = (4,12); self.action_count = 4
        self.start_state = np.ravel_multi_index((3,0), self.shape)
        self.goal_state  = np.ravel_multi_index((3,11), self.shape)
        self.cliff = set(np.ravel_multi_index((3,c), self.shape) for c in range(1,11))
    def step(self, state, action):
        r,c = np.unravel_index(state, self.shape)
        if action == 2: r = min(r+1,3)     # Down
        elif action == 3: c = max(c-1,0)   # Left
        elif action == 0: r = max(r-1,0)   # Up
        elif action == 1: c = min(c+1,11)  # Right
        ns = np.ravel_multi_index((r,c), self.shape)
        if ns in self.cliff: return self.start_state, -100.0, False
        if ns == self.goal_state: return ns, -1.0, True
        return ns, -1.0, False

class TaxiModel(BaseEnvModel):
    """
    Deterministic model from Gym's transition table (env.P).
    State encoding (Gym): s = (((row*5 + col)*5 + pass_loc)*4 + dest)
    Actions: 0=South,1=North,2=East,3=West,4=Pickup,5=Dropoff
    """
    LOCS = [(0,0), (0,4), (4,0), (4,3)]  # R,G,Y,B
    def __init__(self):
        env = gym.make("Taxi-v3")
        # unwrap .env if needed
        base = getattr(env, "env", env)
        self.P = base.P
        self.action_count = base.action_space.n
        self.nS = base.observation_space.n
        env.close()

    @staticmethod
    def decode(s: int):
        dest = s % 4; s //= 4
        pass_loc = s % 5; s //= 5
        col = s % 5; s //= 5
        row = s
        return row, col, pass_loc, dest

    def step(self, state, action):
        trans = self.P[state][action]
        # Each entry: (prob, next_state, reward, terminated) or (..., terminated, truncated)
        best = max(trans, key=lambda t: t[0])
        if len(best) == 4:
            _, ns, r, done = best
        else:
            _, ns, r, term, trunc = best
            done = term or trunc
        return ns, float(r), bool(done)

# -------------
# MCTS Node
# -------------
class Node:
    def __init__(self, state: Any, action_count: int, parent=None, action_taken: Optional[int]=None):
        self.state, self.parent, self.action_taken = state, parent, action_taken
        self.N, self.W = 0, 0.0
        self.children: Dict[int, "Node"] = {}
        self.untried_actions = list(range(action_count)); random.shuffle(self.untried_actions)
    def q_value(self) -> float: return self.W / self.N if self.N>0 else 0.0
    def last_token(self): 
        if self.parent is None or self.action_taken is None: return None
        return (self.parent.state, self.action_taken)

# -------------------------------------
# RaCTSMiner (NPMIÃ—IDF, adaptive n, smoothing, context)
# -------------------------------------
class RaCTSMiner:
    def __init__(self, model: BaseEnvModel, decay=0.997, buffer=4096, top_q=0.85,
                 min_support=3, idf_cap=3.5, smooth=0.25, prune_thresh=1e-3, max_grams=200_000):
        self.model=model; self.decay=decay
        self.c_pos=collections.Counter(); self.c_all=collections.Counter()
        self.pos_total=0.0; self.all_total=0.0
        self.buf_scores=[]; self.buf_cap=buffer; self.top_q=top_q; self.threshold=None
        self.smooth=smooth; self.eps=1e-12; self.min_support=min_support; self.idf_cap=idf_cap
        self.prune_thresh=prune_thresh; self.max_grams=max_grams
        self.pos_updates=0; self.avg_len=0.0
        # base n by map size
        area=16
        if isinstance(model, FrozenLakeModel): area=model.map_size**2
        elif isinstance(model, CliffWalkingModel): area=np.prod(model.shape)
        elif isinstance(model, TaxiModel): area=500
        self.n0 = 1 + (area>=32) + (area>=64)

    def _n_max(self): return max(1, min(3, int(self.n0 + (self.avg_len>=20))))

    def _decay_all(self):
        self.pos_total *= self.decay; self.all_total *= self.decay
        for d in (self.c_pos, self.c_all):
            for k in list(d.keys()):
                d[k] *= self.decay
                if d[k] < self.prune_thresh: del d[k]
        if len(self.c_all) > self.max_grams:
            drop = len(self.c_all)-self.max_grams
            for k,_ in self.c_all.most_common()[:-drop-1:-1]:
                self.c_all.pop(k, None); self.c_pos.pop(k, None)

    def _update_threshold(self):
        if len(self.buf_scores) >= 50:
            s=sorted(self.buf_scores); k=max(0,min(len(s)-1,int(self.top_q*len(s))-1))
            self.threshold=s[k]

    def _grams_from_trace(self, trace: List[Tuple[int,int]]):
        grams=set(); L=len(trace); nmax=self._n_max()
        for n in range(1,nmax+1):
            for i in range(L-n+1): grams.add(tuple(trace[i:i+n]))
        return grams

    def update(self, trace: List[Tuple[int,int]], total_reward: float):
        self.avg_len = (0.9*self.avg_len + 0.1*len(trace)) if self.avg_len>0 else len(trace)
        self.buf_scores.append(total_reward); 
        if len(self.buf_scores)>self.buf_cap: self.buf_scores.pop(0)
        self._update_threshold()
        grams=self._grams_from_trace(trace); self._decay_all()
        for g in grams: self.c_all[g]+=1.0
        self.all_total+=1.0
        high = (self.threshold is None and total_reward>0) or (self.threshold is not None and total_reward>=self.threshold)
        if high:
            for g in grams: self.c_pos[g]+=1.0
            self.pos_total+=1.0; self.pos_updates+=1

    def _npmi(self,g):
        s=self.smooth
        p_high=(self.pos_total+s)/(self.all_total+2*s)
        p_g   =(self.c_all[g]+s   )/(self.all_total+2*s)
        p_both=(self.c_pos[g]+s   )/(self.all_total+2*s)
        if p_both<=self.eps: return -1.0
        pmi = math.log(p_both/(p_g*p_high+self.eps))
        return pmi/(-math.log(p_both+self.eps))

    def _idf(self,g):
        df=self.c_all[g]
        if df<self.min_support: return 0.0
        N=max(self.all_total,1.0)
        return min(math.log((N+1)/(df+1)), self.idf_cap)

    def _ctx_weight(self, node: "Node") -> float:
        # FrozenLake: closer to goal
        if isinstance(self.model, FrozenLakeModel):
            n=self.model.map_size; r,c=divmod(node.state,n); d=abs((n-1)-r)+abs((n-1)-c); d0=max(1,2*n-2)
            return 1.0 + 0.5*(1 - d/d0)
        # CliffWalking: higher rows safer
        if isinstance(self.model, CliffWalkingModel):
            r,_=np.unravel_index(node.state, self.model.shape)
            return 1.0 + 0.1*(3 - r)
        # Taxi: distance to passenger (if not onboard) else to destination
        if isinstance(self.model, TaxiModel):
            row,col,pl,dst = TaxiModel.decode(node.state)
            if pl==4:
                dr,dc = TaxiModel.LOCS[dst]
            else:
                dr,dc = TaxiModel.LOCS[pl]
            d = abs(dr-row)+abs(dc-col); d0 = 8  # max in 5x5
            return 1.0 + 0.4*(1 - d/d0)
        return 1.0

    def calculate_prior(self, node: "Node", action: int) -> float:
        s=node.state; score=0.0
        g1=((s,action),); x1=self._npmi(g1); 
        if x1>0: score += x1*self._idf(g1)
        prev=node.last_token()
        if prev is not None:
            g2=(prev,(s,action)); x2=self._npmi(g2)
            if x2>0: score += x2*self._idf(g2)
        if prev is not None and node.parent and node.parent.parent:
            prev2 = node.parent.parent
            prev_tok = prev2.last_token()
            if prev_tok is not None:
                g3=(prev_tok,prev,(s,action)); x3=self._npmi(g3)
                if x3>0: score += x3*self._idf(g3)
        return score * self._ctx_weight(node)

# -------------------
# MCTS Solver (PUCT + safety + mini-duel)
# -------------------
class Node:  # re-define to ensure type hints above still valid
    def __init__(self, state: Any, action_count: int, parent=None, action_taken: Optional[int]=None):
        self.state, self.parent, self.action_taken = state, parent, action_taken
        self.N, self.W = 0, 0.0
        self.children: Dict[int, "Node"] = {}
        self.untried_actions = list(range(action_count)); random.shuffle(self.untried_actions)
    def q_value(self) -> float: return self.W/self.N if self.N>0 else 0.0
    def last_token(self):
        if self.parent is None or self.action_taken is None: return None
        return (self.parent.state, self.action_taken)

class MCTSSolver:
    def __init__(self, model: BaseEnvModel, max_simulations_per_move=200):
        self.model=model; self.action_count=model.action_count; self.max_sims=max_simulations_per_move

    def _simulate_from(self, state, gamma=0.99, max_depth=100):
        done=False; total=0.0; disc=1.0; cur=state
        for _ in range(max_depth):
            if done: break
            a=random.randrange(self.action_count)
            cur,r,done=self.model.step(cur,a)
            total += disc*r; disc *= gamma
        return total

    def _select_and_expand(self, root, c_uct, miner, c_puct, beta, trust_margin, warm_sims, sims_done):
        node, path = root, [root]
        while not node.untried_actions and node.children:
            # best UCT
            best_v, best_v_val = None, -1e9
            for a,ch in node.children.items():
                Q = ch.q_value(); U = c_uct*math.sqrt(max(0.0, math.log(node.N+1))/(ch.N+1e-9))
                val = Q+U
                if val>best_v_val: best_v_val, best_v = val, (a,ch)
            # best PUCT
            if miner is not None and beta>0 and sims_done>=warm_sims:
                acts=list(node.children.keys())
                raw=[miner.calculate_prior(node,a) for a in acts]
                m=max(raw) if raw else 0.0
                exps=[math.exp(beta*(v-m)) for v in raw]; Z=sum(exps) or 1.0
                P={a:e/Z for a,e in zip(acts,exps)}
            else:
                P={a:1.0/len(node.children) for a in node.children}
            best_p, best_p_val = None, -1e9
            for a,ch in node.children.items():
                Q=ch.q_value(); U=c_puct*P[a]*math.sqrt(max(1.0,node.N))/(1.0+ch.N)
                val=Q+U
                if val>best_p_val: best_p_val, best_p = val, (a,ch)
            # safety
            if (miner is not None and beta>0 and sims_done>=warm_sims and best_p_val>=best_v_val+trust_margin):
                a_sel, node = best_p
            else:
                a_sel, node = best_v
            path.append(node)
        # expand
        if node.untried_actions:
            u_scores=[]
            for a in node.untried_actions:
                Qp=node.q_value(); U=c_uct*math.sqrt(max(0.0, math.log(node.N+1))/1.0)
                u_scores.append((a,Qp+U))
            a_v=max(u_scores, key=lambda x:x[1])[0]; cand=[a_v]
            if miner is not None and beta>0:
                a_p=max([(a, miner.calculate_prior(node,a)) for a in node.untried_actions], key=lambda x:x[1])[0]
                if a_p!=a_v: cand.append(a_p)
            elif len(node.untried_actions)>1:
                cand.append(random.choice([a for a in node.untried_actions if a!=a_v]))
            a_exp = random.choice(node.untried_actions) if (random.random()<0.1) else cand[0]
            node.untried_actions.remove(a_exp)
            ns,_,_ = self.model.step(node.state, a_exp)
            child=Node(ns, self.action_count, parent=node, action_taken=a_exp)
            node.children[a_exp]=child; path.append(child)
        return path

    def choose_move(self, start_state, *, mode='vanilla', miner: Optional[RaCTSMiner]=None,
                    c_uct=1.4, c_puct=1.25, beta_max=2.0, beta_start_pos=3, beta_full_pos=9,
                    trust_margin=0.05, warm_sims=50, mini_duel=True, duel_extra_sims=6):
        root=Node(start_state, self.action_count); sims_done=0
        def beta_eff(pos_updates): 
            x=(pos_updates-beta_start_pos)/max(1.0,(beta_full_pos-beta_start_pos)); x=max(0.0,min(1.0,x))
            return beta_max*x
        while sims_done < self.max_sims:
            beta = beta_eff(miner.pos_updates) if (mode=='ramcts' and miner is not None) else 0.0
            path = self._select_and_expand(root, c_uct, miner if mode=='ramcts' else None,
                                           c_puct, beta, trust_margin, warm_sims, sims_done)
            R = self._simulate_from(path[-1].state)
            for n in path: n.N += 1; n.W += R
            sims_done += 1
        if not root.children: return random.randrange(self.action_count)
        # mini-duel
        if mode=='ramcts' and miner is not None and mini_duel:
            acts=list(root.children.keys())
            uct_vals={}; 
            for a,ch in root.children.items():
                Q=ch.q_value(); U=c_uct*math.sqrt(max(0.0, math.log(root.N+1))/(ch.N+1e-9))
                uct_vals[a]=Q+U
            a_v=max(uct_vals, key=uct_vals.get)
            raw=[miner.calculate_prior(root,a) for a in acts]
            m=max(raw) if raw else 0.0
            exps=[math.exp((beta_eff(miner.pos_updates) or 0.0)*(v-m)) for v in raw]; Z=sum(exps) or 1.0
            P={a:e/Z for a,e in zip(acts,exps)}; a_p=max(acts, key=lambda a:P[a])
            if a_p!=a_v:
                for a in (a_v,a_p):
                    ch=root.children[a]
                    for _ in range(duel_extra_sims):
                        R=self._simulate_from(ch.state)
                        ch.N+=1; ch.W+=R
                return a_p if root.children[a_p].q_value() >= root.children[a_v].q_value() else a_v
        return max(root.children, key=lambda a: root.children[a].N)

# --------------------
# Q-Learning
# --------------------
class QLearningSolver:
    def __init__(self, env):
        self.env=env
        self.q=np.zeros([env.observation_space.n, env.action_space.n])
        self.alpha=0.1; self.gamma=0.99; self.eps=1.0; self.eps_min=0.01; self.eps_decay=0.999
    def choose_move(self, s):
        return self.env.action_space.sample() if random.random()<self.eps else int(np.argmax(self.q[s]))
    def update(self, s,a,r,ns):
        self.q[s,a] = (1-self.alpha)*self.q[s,a] + self.alpha*(r + self.gamma*np.max(self.q[ns]))
    def decay(self): self.eps = max(self.eps_min, self.eps*self.eps_decay)

# --------------------
# Episode Runner
# --------------------
def run_benchmark_episode(agent, env, solver_type, model: BaseEnvModel, miner: Optional[RaCTSMiner]=None,
                          max_steps=200, c_vanilla=1.4, c_puct=1.25,
                          trust_margin=0.05, warm_sims=50, beta_max=2.0,
                          beta_start=3, beta_full=9, mini_duel=True, duel_extra_sims=6) -> bool:
    s,_ = env.reset(seed=random.randint(0,10**9))
    trace=[]; total=0.0; success=False
    for _ in range(max_steps):
        if solver_type=='q_learning':
            a=agent.choose_move(s)
        elif solver_type=='vanilla':
            a=agent.choose_move(s, mode='vanilla', c_uct=c_vanilla)
        else:
            a=agent.choose_move(s, mode='ramcts', miner=miner, c_uct=c_vanilla, c_puct=c_puct,
                                trust_margin=trust_margin, warm_sims=warm_sims,
                                beta_max=beta_max, beta_start_pos=beta_start, beta_full_pos=beta_full,
                                mini_duel=mini_duel, duel_extra_sims=duel_extra_sims)
        trace.append((s,a))
        s,r,done,_,_=env.step(a)
        total += r
        if done:
            success = True
            break
    if solver_type=='q_learning':
        agent.update(trace[-1][0], trace[-1][1], r, s); agent.decay()
    elif solver_type=='ramcts' and miner is not None:
        miner.update(trace, total)
    # Success rule per env: Taxi & FrozenLake use 'done'; CliffWalking uses reward level
    env_id = env.spec.id
    if "CliffWalking" in env_id:
        return total > -17  # lenient threshold
    else:
        return success

# --------------------
# Driver per method
# --------------------
def run_solver(solver_type, env_name, map_name=None, max_episodes=1000, success_streak=10,
               sims_small=80, sims_large=260, duel_extra_sims=6):
    if 'FrozenLake' in env_name:
        env = gym.make(env_name, map_name=map_name, is_slippery=False)
        model = FrozenLakeModel(map_name)
        sims = sims_small if map_name=="4x4" else sims_large
    elif 'Taxi' in env_name:
        env = gym.make("Taxi-v3")
        model = TaxiModel()
        sims = 220  # a bit more budget than FL-8x8
    else:
        env = gym.make("CliffWalking-v1")
        model = CliffWalkingModel()
        sims = 150

    if solver_type=='q_learning':
        agent = QLearningSolver(env); miner=None
    else:
        agent = MCTSSolver(model, max_simulations_per_move=sims)
        miner = RaCTSMiner(model) if solver_type=='ramcts' else None

    recent = collections.deque(maxlen=success_streak)
    for ep in range(1, max_episodes+1):
        ok = run_benchmark_episode(agent, env, solver_type, model, miner,
                                   max_steps=200, c_vanilla=1.4, c_puct=1.25,
                                   trust_margin=0.05, warm_sims=50,
                                   beta_max=2.0, beta_start=3, beta_full=9,
                                   mini_duel=(solver_type=='ramcts'), duel_extra_sims=duel_extra_sims)
        recent.append(1.0 if ok else 0.0)
        avg = sum(recent)/len(recent)
        print(f"\r{solver_type.title():<10} Ep {ep:<4} StreakAvg({len(recent)}/{success_streak}): {avg:.2f}", end="")
        solved = (len(recent)==success_streak and avg>0.9) if ('FrozenLake' in env_name or 'Taxi' in env_name) \
                 else (len(recent)==success_streak and avg>0.5)
        if solved:
            print(f"\nSolved in {ep} episodes."); env.close(); return True, ep
    print(f"\nFailed to solve in {max_episodes} episodes."); env.close(); return False, max_episodes

# --------------------
# Full Benchmark
# --------------------
def run_full_benchmark():
    ALL=[]
    SUITE=[("FrozenLake-v1","4x4"), ("FrozenLake-v1","8x8"), ("Taxi-v3",None)]
    METHODS=["q_learning","vanilla","ramcts"]
    for env_name, map_size in SUITE:
        display_name = f"{env_name} ({map_size})" if map_size else env_name
        print("\n"+"="*60); print(f"BENCHMARKING ON {display_name}"); print("="*60+"\n")
        for method in METHODS:
            print(f"--- {method.replace('_',' ').title()} on {display_name} ---")
            solved, eps = run_solver(method, env_name, map_size,
                                     max_episodes=2000 if method=='q_learning' else 400,
                                     success_streak=10,
                                     sims_small=90, sims_large=280, duel_extra_sims=6)
            ALL.append({'Environment':display_name,'Method':method.replace('_',' ').title(),
                        'Solved':'Yes' if solved else 'No',
                        'Episodes to Solve': eps if solved else 'N/A'})
            print("-"*40)
    df=pd.DataFrame(ALL)
    print("\nFinal Summary:"); 
    try:
        from IPython.display import display; display(df)
    except: print(df.to_string(index=False))
    return df

# Run
_ = run_full_benchmark()